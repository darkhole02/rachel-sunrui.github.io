---
title: 'Approximate Inference'
date: 2018-08-03
permalink: /posts/2018/08/mcmc/
tags:
  - pgm
  - monte carlo
categories:
  - Machine learning
---

(Summarized from the lecture notes from cmu spring17-10708, Eric Xing)

### Motivation
When we want to know $$E_p[f(x)]$$, where $$x \sim p(x)$$, we need to know the distribution of $$x$$. One way is to use a sampled base average to approximate the expectation $$E_p[f(x)] = \frac{1}{N} \sum_{t=1}^{N}f(x^{(t)})$$. However a few challenges remain that:
* how to draw samples from a given dist. (not all distributions can be trivially sampled)?
* how to make better use of the samples (not all sample are useful, or eqally useful, see an example later)?
* how to know we've sampled enough? (For a model with hundreds or more variables, rare events will be very hard to garner evough samples even after a long time or sampling)

So we have a few methods belong to the Monte Carlo methods:
* Rejection sampling: Create samples like direct sampling, only count samples which is consistent with given evidences.
* Likelihood weighting: Sample variables and calculate evidence weight. Only create the samples which support the evidences.
* Markov chain Monte Carlo (MCMC): Metropolis-Hasting, Gibbs

### Rejection sampling:
How to generate samples given a distribution $$p(x)$$? How to convert samples from a Uniform[0,1] generator to samples that follows distribution $$p(x)$$? One way is to compute cdf $$h(y)$$, which is a uniform non-decreasing function in [0,1]. Then for every sample $$u$$ from Uniform[0,1], we can get $$x = h^{-1}(u)$$ that $$x \sim p(x)$$. But this wonâ€™t work if $$\hat{p}(x)$$ is not normalized. This leads to the idea of rejection sampling:
1. Come up with a probability distribution $$Q(x)$$ (usually Gaussian) that we can easily draw samples from.
2. Find a constant $$k$$ such that $$\frac{\hat{p}(x)}{kQ(x)} < 1$$.
3. Draw a sample $$x_0$$ from $$Q$$
4. Accept a sample with probability $$\frac{\hat{p}(x)}{kQ(x)}$$: sample $$u_0$$ from Uniform[0,$$kQ(x_0)$$], accept the sample if $$u_0 < \hat{p}(x_0)$$
![rs]({{site.url}}{{site.baseurl}}/assets/images/rej_sample.jpeg)

And it can be proved that the accpected samples follows $$p(x)$$.
**Pitfalls:** 
* Scale badly with dimensionality (acceptance rate exponentially decreases)
* If $$Q(x)$$ is chosen badly (if $$p$$ and $$Q$$ both are Gaussian), acceptance rate is really low
