---
title: 'Studies notes for optimization for deep learning'
date: 2020-02-10
permalink: /posts/2020/02/opnn/
tags:
  - neural networks
categories:
  - Machine Learning
---

` Sun, Ruoyu. "Optimization for deep learning: theory and algorithms." arXiv preprint arXiv:1912.08957 (2019).`

## Introduction
*  One major goal is to understanding how the neural-net structure (the parameterization by concatenation of many variables) aï¬€ects the design and analysis of optimization algorithms, which can potentially go beyond supervised learning. 
*  **Prior knowledge on optimization methods and basic theory will be very helpful (see, e.g., [24, 200, 30] for preparation).**
*   The test error can be decomposed into representation error, optimization error and generalization error, corresponding to the error caused by each of the three steps. 
* **I cannot understand chapter 3.2 Need to review My optimization course**

## Neural-net Specific tricks
This mostly talks about how to deal with Gradient Explosion/Vanishing by using carefull initialization, nomalization (batchnorm more specifically), and good architecture (ResNet more specifically)

*  verify that the local Lipschitz constant of the gradient (the maximum eigenvalue of the Hessian) is rather small in practical neural network training, partially due to careful initialization. 
