---
title: 'Neural Networks'
date: 2018-07-06
permalink: /posts/2018/07/nn/
tags:
  - neural networks
categories:
  - Machine Learning
---

## Graident descent
### The limitations
* Assume we find the global minimum of The loss function and the optimal classcification boundary is learnable by the network
    * The loss function is just a proxy for the classcification error, the minimum for the loss is not the best boundary.
    * The nn is a high bias low variance classcifier.(Outlier would not change the boundary.) But perceptron learning rule is sensitive to outliers.
* It can be stuck at saddle points(more common than local optimal)

### Convergence
* One dimensional: we can diverge if $$\eta \ge 2 \eta_{opt}$$
* Multi-variant: learning rate is the same for all dimension, we can diverge if $$\eta \ge 2 min_i \eta_{i,opt}$$
* Convergence is slow when condition number is small

### Newton's method
* Difficult to invert(too large)
* Not convex function hessian are not psd
* Use quasi-newton's method.
* Less popular for large nn

### Learning rate
* The learning rate could be larger than 2 * opt at the beginning.
* The same learning rate at all directions cause problem
* Rprop: first order, no need for convexity assumption. But not useful for minibatch gradient descent
* RMSprop: improve based on Rprop, use the running average of the magnitude of the gradient. Running averages grow, then scale down the gradient/learning rate.
* Quickprop: secnond derivative at one direction
* Rprop and Quickprop treat each dimension seperately
* Adam(Adaptive moment estimation): 

### Momentum
* Keeps a running average of the gradient, usually keeps to be 0.9old, 0.1new.
* Move more to the direction with more loss drop. If the direction oscilates, then the running average should be small. Then move less to that direction.
* Nestorov's accelerated gradient: first extended the previous step and calcualted the gradient at the new place

### Initialization





