---
title: 'Neural Networks'
date: 2018-07-06
permalink: /posts/2018/07/nn/
tags:
  - neural networks
categories:
  - Machine Learning
---

## Graident descent
### The limitations
* Assume we find the global minimum of The loss function and the optimal classcification boundary is learnable by the network
    * The loss function is just a proxy for the classcification error, the minimum for the loss is not the best boundary.
    * The nn is a high bias low variance classcifier.(Outlier would not change the boundary.) But perceptron learning rule is sensitive to outliers.
* It can be stuck at saddle points(more common than local optimal)

### Convergence
* One dimensional: we can diverge if $$\eta \ge 2 \eta_{opt}$$
* Multi-variant: learning rate is the same for all dimension, we can diverge if $$\eta \ge 2 min_i \eta_{i,opt}$$
* Convergence is slow when condition number is small

### Newton's method
* Difficult to invert(too large)
* Not convex function hessian are not psd
* Use quasi-newton's method.
* Less popular for large nn

### Learning rate
* The learning rate could be larger than 2 * opt at the beginning.
* The same learning rate at all directions cause problem
* Rprop: first order, no need for convexity assumption. But not useful for minibatch gradient descent
* RMSprop: improve based on Rprop, use the running average of the magnitude of the gradient. Running averages grow, then scale down the gradient/learning rate.
* Quickprop: secnond derivative at one direction
* Rprop and Quickprop treat each dimension seperately
* Adam(Adaptive moment estimation): 

### Momentum
* Keeps a running average of the gradient, usually keeps to be 0.9old, 0.1new.
* Move more to the direction with more loss drop. If the direction oscilates, then the running average should be small. Then move less to that direction.
* Nestorov's accelerated gradient: first extended the previous step and calcualted the gradient at the new place

### Initialization
The general idea is making the variance of the output close to 1. We usually normalize the input variance to be zero mean and variance 1 and the mean of the weights are initialized to be 0, and based on this, the variance of the weights can be calcualted.
* **LeCun and Xavier initialization**: More suitable for linear/sigmoid activation. LeCun only thinks about the influence of the forward pass, so $$Var(w) = 1/fanIn$$, and Xavier considers both forward and backward pass, so $$Var(w) = \frac{2}{fanIn+fanOut}$$
* **Kaiming initialization**: ReLU activation was used, so $$Var(w) = 2/fanIn$$ or $$Var(w) = 2/fanOut$$
* **Dynamical isometry**: This line of research looks at the the singular values of the input-output Jacobian matrix and trying to make that all 1. (My understanding is that in this way, the gradent of output over input is on the same scale, so we can use one learning rate to update all teh output feature and have a good, stable reasult). We could use all orthogonal matrices as the inilization weights for sigmoid activation, but dynamical isometry properity cannot be observed if using orthogonal matrices and ReLU.

### Deconvolution
[Good visualization](https://github.com/vdumoulin/conv_arithmetic)




